{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    " \n",
    "generator=pipeline('text-generation',model='gpt2')\n",
    "\n",
    "\n",
    "text1 = generator('accounting ', max_length=50)[0]['generated_text']\n",
    "text2 = generator('Graphic designer', max_length=50)[0]['generated_text']\n",
    "text3 = generator('mecanical engineering ', max_length=50)[0]['generated_text']\n",
    "text4 = generator('artificial intelligence', max_length=50)[0]['generated_text']\n",
    "\n",
    "documents = [text1 + \" \" + text2 + \" \" + text3 + \" \" + text4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: accounting  with a new password (or just have a new password that will not be reset on boot) is done in sudo userconfig.\n",
      "sudo userconfig.sudo\n",
      "# -e /users/test{}/dev/null\n",
      "Document 2: Graphic designer: Jef Gifford\n",
      "\n",
      "Jef Gifford created his design for the second season of \"The Last Temptation of Christ.\" One of his favorites, it's a work of \"literary fiction.\" When you\n",
      "Document 3: mecanical engineering  in the US since 1992.  I was just beginning to learn about their processes of manufacture and what are they supposed to do.  I think the general rule of thumb for their work is that if you are going to\n",
      "Document 4: artificial intelligence has shown it, or even that it was created as an experiment on behalf of a university in the US and thus not \"real\" work.\n",
      "\n",
      "So what would you do, if you were a person who were \"committed\n"
     ]
    }
   ],
   "source": [
    "print(\"Document 1:\", text1)\n",
    "print(\"Document 2:\", text2)\n",
    "print(\"Document 3:\", text3)\n",
    "print(\"Document 4:\", text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess the documents\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    current_token = \"\"\n",
    "    for char in text:\n",
    "        if char.isalnum():  # Check if the character is alphanumeric\n",
    "            current_token += char\n",
    "        else:\n",
    "            if current_token:\n",
    "                tokens.append(current_token.lower())  # Add the token to the list\n",
    "                current_token = \"\"\n",
    "            if char.strip():  # Check if the character is not whitespace\n",
    "                tokens.append(char)  # Add non-alphanumeric characters as separate tokens\n",
    "    if current_token:\n",
    "        tokens.append(current_token.lower())  # Add the last token if any\n",
    "    return tokens\n",
    "\n",
    "def preprocess_document(doc):\n",
    "    # Remove punctuation\n",
    "    doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenization\n",
    "    tokens = tokenize(doc)\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "preprocessed_documents = [preprocess_document(doc) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 3: Calculate TF (Term Frequency)\n",
    "tf_matrix = []\n",
    "for doc_tokens in preprocessed_documents:\n",
    "    doc_word_count = len(doc_tokens)\n",
    "    word_freq = Counter(doc_tokens)\n",
    "    tf = {word: word_freq[word] / doc_word_count for word in word_freq}\n",
    "    tf_matrix.append(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate IDF (Inverse Document Frequency)\n",
    "total_docs = len(preprocessed_documents)\n",
    "word_doc_freq = {}\n",
    "for doc_tokens in preprocessed_documents:\n",
    "    for word in set(doc_tokens):\n",
    "        word_doc_freq[word] = word_doc_freq.get(word, 0) + 1\n",
    "\n",
    "idf = {word: math.log(total_docs / (word_doc_freq[word] + 1)) for word in word_doc_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF for each document:\n",
      "Document 1 : {'accounting': -0.01050223000848402, 'new': -0.02100446001696804, 'password': -0.02100446001696804, 'reset': -0.01050223000848402, 'boot': -0.01050223000848402, 'done': -0.01050223000848402, 'sudo': -0.02100446001696804, 'userconfig': -0.01050223000848402, 'userconfigsudo': -0.01050223000848402, 'e': -0.01050223000848402, 'userstestdevnull': -0.01050223000848402, 'graphic': -0.01050223000848402, 'designer': -0.01050223000848402, 'jef': -0.02100446001696804, 'gifford': -0.02100446001696804, 'created': -0.02100446001696804, 'design': -0.01050223000848402, 'second': -0.01050223000848402, 'season': -0.01050223000848402, 'last': -0.01050223000848402, 'temptation': -0.01050223000848402, 'christ': -0.01050223000848402, 'one': -0.01050223000848402, 'favorite': -0.01050223000848402, 'work': -0.03150669002545206, 'literary': -0.01050223000848402, 'fiction': -0.01050223000848402, 'mecanical': -0.01050223000848402, 'engineering': -0.01050223000848402, 'u': -0.02100446001696804, 'since': -0.01050223000848402, '1992': -0.01050223000848402, 'wa': -0.02100446001696804, 'beginning': -0.01050223000848402, 'learn': -0.01050223000848402, 'process': -0.01050223000848402, 'manufacture': -0.01050223000848402, 'supposed': -0.01050223000848402, 'think': -0.01050223000848402, 'general': -0.01050223000848402, 'rule': -0.01050223000848402, 'thumb': -0.01050223000848402, 'going': -0.01050223000848402, 'artificial': -0.01050223000848402, 'intelligence': -0.01050223000848402, 'ha': -0.01050223000848402, 'shown': -0.01050223000848402, 'even': -0.01050223000848402, 'experiment': -0.01050223000848402, 'behalf': -0.01050223000848402, 'university': -0.01050223000848402, 'thus': -0.01050223000848402, 'real': -0.01050223000848402, 'would': -0.01050223000848402, 'person': -0.01050223000848402, 'committed': -0.01050223000848402}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Multiply TF by IDF to get TFIDF\n",
    "tfidf_matrix = []\n",
    "for tf in tf_matrix:\n",
    "    tfidf = {word: tf[word] * idf[word] for word in tf}\n",
    "    tfidf_matrix.append(tfidf)\n",
    "\n",
    "# Output TFIDF for each document\n",
    "print(\"TFIDF for each document:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(\"Document\", i+1, \":\", tfidf_matrix[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
